{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom modules\n",
    "from data_loader import AadhaarDataLoader\n",
    "from preprocessing import AadhaarDataPreprocessor\n",
    "from visualization import AadhaarVisualizer\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa11e32f",
   "metadata": {},
   "source": [
    "## 1. Initialize Data Loader and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da0370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "BASE_PATH = '/Users/satyamsharma/Satverse AI/UIDAI Data Hackathon 2026'\n",
    "loader = AadhaarDataLoader(BASE_PATH)\n",
    "preprocessor = AadhaarDataPreprocessor()\n",
    "visualizer = AadhaarVisualizer(output_dir='../outputs/figures')\n",
    "\n",
    "print(\"✓ Initialized data loader, preprocessor, and visualizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c073cf9",
   "metadata": {},
   "source": [
    "## 2. Dataset Information\n",
    "\n",
    "Get overview of available datasets before loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset information\n",
    "info = loader.get_dataset_info()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_type, details in info.items():\n",
    "    print(f\"\\n{dataset_type.upper()}\")\n",
    "    print(f\"  Number of files: {details['num_files']}\")\n",
    "    print(f\"  Total rows: {details['total_rows']:,}\")\n",
    "    print(f\"  Files:\")\n",
    "    for file in details['files']:\n",
    "        print(f\"    - {file}\")\n",
    "\n",
    "total_rows = sum(d['total_rows'] for d in info.values())\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TOTAL RECORDS ACROSS ALL DATASETS: {total_rows:,}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc92b92",
   "metadata": {},
   "source": [
    "## 3. Load Datasets\n",
    "\n",
    "Load all datasets. We'll start with a sample for faster processing, then can load full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ad2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets - adjust sample_frac as needed\n",
    "# sample_frac=0.1 means 10% of data (faster for initial exploration)\n",
    "# sample_frac=1.0 means full data (use for final analysis)\n",
    "\n",
    "SAMPLE_FRACTION = 0.2  # Start with 20% of data\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "print(f\"Sample fraction: {SAMPLE_FRACTION * 100}%\\n\")\n",
    "\n",
    "datasets = loader.load_all_datasets(sample_frac=SAMPLE_FRACTION)\n",
    "\n",
    "enrolment_df = datasets['enrolment']\n",
    "demographic_df = datasets['demographic']\n",
    "biometric_df = datasets['biometric']\n",
    "\n",
    "print(\"\\n✓ All datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc47bd49",
   "metadata": {},
   "source": [
    "## 4. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f635151d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enrolment dataset\n",
    "print(\"=\"*80)\n",
    "print(\"ENROLMENT DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {enrolment_df.shape}\")\n",
    "print(f\"\\nColumns: {enrolment_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(enrolment_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(enrolment_df.dtypes)\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(enrolment_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db553aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demographic dataset\n",
    "print(\"=\"*80)\n",
    "print(\"DEMOGRAPHIC UPDATE DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {demographic_df.shape}\")\n",
    "print(f\"\\nColumns: {demographic_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(demographic_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(demographic_df.dtypes)\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(demographic_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d30d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biometric dataset\n",
    "print(\"=\"*80)\n",
    "print(\"BIOMETRIC UPDATE DATASET\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Shape: {biometric_df.shape}\")\n",
    "print(f\"\\nColumns: {biometric_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(biometric_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(biometric_df.dtypes)\n",
    "print(f\"\\nBasic statistics:\")\n",
    "display(biometric_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c1032",
   "metadata": {},
   "source": [
    "## 5. Data Validation\n",
    "\n",
    "Check data quality issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa1aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate enrolment data\n",
    "print(\"VALIDATING ENROLMENT DATA\")\n",
    "print(\"=\"*80)\n",
    "enrolment_validation = preprocessor.validate_data(enrolment_df, 'enrolment')\n",
    "\n",
    "for key, value in enrolment_validation.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de5107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate demographic data\n",
    "print(\"\\nVALIDATING DEMOGRAPHIC DATA\")\n",
    "print(\"=\"*80)\n",
    "demographic_validation = preprocessor.validate_data(demographic_df, 'demographic')\n",
    "\n",
    "for key, value in demographic_validation.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac24712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate biometric data\n",
    "print(\"\\nVALIDATING BIOMETRIC DATA\")\n",
    "print(\"=\"*80)\n",
    "biometric_validation = preprocessor.validate_data(biometric_df, 'biometric')\n",
    "\n",
    "for key, value in biometric_validation.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c21b205",
   "metadata": {},
   "source": [
    "## 6. Data Cleaning\n",
    "\n",
    "Clean and standardize all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean datasets\n",
    "print(\"Cleaning datasets...\")\n",
    "\n",
    "enrolment_clean = preprocessor.clean_data(enrolment_df, 'enrolment')\n",
    "demographic_clean = preprocessor.clean_data(demographic_df, 'demographic')\n",
    "biometric_clean = preprocessor.clean_data(biometric_df, 'biometric')\n",
    "\n",
    "print(f\"\\nEnrolment: {len(enrolment_df):,} → {len(enrolment_clean):,} rows\")\n",
    "print(f\"Demographic: {len(demographic_df):,} → {len(demographic_clean):,} rows\")\n",
    "print(f\"Biometric: {len(biometric_df):,} → {len(biometric_clean):,} rows\")\n",
    "\n",
    "print(\"\\n✓ Data cleaning completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41c462",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Add derived features for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a711f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add derived features\n",
    "print(\"Adding derived features...\")\n",
    "\n",
    "enrolment_enhanced = preprocessor.add_derived_features(enrolment_clean, 'enrolment')\n",
    "demographic_enhanced = preprocessor.add_derived_features(demographic_clean, 'demographic')\n",
    "biometric_enhanced = preprocessor.add_derived_features(biometric_clean, 'biometric')\n",
    "\n",
    "print(\"\\nEnrolment enhanced columns:\")\n",
    "print(enrolment_enhanced.columns.tolist())\n",
    "\n",
    "print(\"\\nDemographic enhanced columns:\")\n",
    "print(demographic_enhanced.columns.tolist())\n",
    "\n",
    "print(\"\\nBiometric enhanced columns:\")\n",
    "print(biometric_enhanced.columns.tolist())\n",
    "\n",
    "print(\"\\n✓ Feature engineering completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b412f0",
   "metadata": {},
   "source": [
    "## 8. Geographic Analysis - Unique Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace4d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze geographic coverage\n",
    "print(\"GEOGRAPHIC COVERAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nUnique States: {enrolment_enhanced['state'].nunique()}\")\n",
    "print(f\"Top 10 States by records:\")\n",
    "print(enrolment_enhanced['state'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\n\\nUnique Districts: {enrolment_enhanced['district'].nunique()}\")\n",
    "print(f\"Top 10 Districts by records:\")\n",
    "print(enrolment_enhanced['district'].value_counts().head(10))\n",
    "\n",
    "print(f\"\\n\\nUnique PIN Codes: {enrolment_enhanced['pincode'].nunique()}\")\n",
    "print(f\"Sample PIN codes:\")\n",
    "print(enrolment_enhanced['pincode'].head(20).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daedb46a",
   "metadata": {},
   "source": [
    "## 9. Temporal Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878e14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal analysis\n",
    "print(\"TEMPORAL COVERAGE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, df in [('Enrolment', enrolment_enhanced), \n",
    "                  ('Demographic', demographic_enhanced), \n",
    "                  ('Biometric', biometric_enhanced)]:\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "    print(f\"  Number of days: {(df['date'].max() - df['date'].min()).days}\")\n",
    "    print(f\"  Records per day (avg): {len(df) / df['date'].nunique():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc6c2ff",
   "metadata": {},
   "source": [
    "## 10. Save Processed Data\n",
    "\n",
    "Save cleaned and enhanced datasets for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97294dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "output_dir = '../outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "enrolment_enhanced.to_parquet(f'{output_dir}/enrolment_processed.parquet', index=False)\n",
    "demographic_enhanced.to_parquet(f'{output_dir}/demographic_processed.parquet', index=False)\n",
    "biometric_enhanced.to_parquet(f'{output_dir}/biometric_processed.parquet', index=False)\n",
    "\n",
    "print(\"✓ Processed datasets saved to outputs/\")\n",
    "print(\"  - enrolment_processed.parquet\")\n",
    "print(\"  - demographic_processed.parquet\")\n",
    "print(\"  - biometric_processed.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c278b71",
   "metadata": {},
   "source": [
    "## 11. Summary\n",
    "\n",
    "**Data Loading Summary:**\n",
    "- Loaded enrolment, demographic, and biometric update datasets\n",
    "- Validated data quality and identified issues\n",
    "- Cleaned data (removed duplicates, invalid dates, negative values)\n",
    "- Added derived features (temporal features, totals, proportions)\n",
    "- Saved processed data for analysis\n",
    "\n",
    "**Next Steps:**\n",
    "1. Exploratory Data Analysis (EDA)\n",
    "2. Temporal and Spatial Analysis\n",
    "3. Anomaly Detection\n",
    "4. Predictive Modeling\n",
    "5. Insights Generation"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
